{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gulendamarici/Netflix-project/blob/main/Churnanalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "zc2S2UIKaMRo",
        "outputId": "ef0edafd-660e-4ee2-cbde-7fb68ad0eb99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/275.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m266.2/275.7 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.7/275.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for lime (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install xgboost lightgbm imbalanced-learn shap lime --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "from imblearn.over_sampling import SMOTE, ADASYN\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    roc_auc_score,\n",
        "    confusion_matrix,\n",
        "    classification_report\n",
        ")\n",
        "\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set(style=\"whitegrid\")\n"
      ],
      "metadata": {
        "id": "G9wccY8W4tmU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dosya adını kendi dosyana göre güncelle\n",
        "file_path = \"/content/sample_data/WA_Fn-UseC_-Telco-Customer-Churn.csv\"\n",
        "\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "print(\"İlk 5 satır:\")\n",
        "display(df.head())\n",
        "\n",
        "print(\"\\nVeri seti bilgisi:\")\n",
        "print(df.info())\n",
        "\n",
        "print(\"\\nChurn dağılımı:\")\n",
        "print(df['Churn'].value_counts(normalize=True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "O_IYYGpY5Aot",
        "outputId": "8992b6bd-5bdd-4e3c-fd07-79dece7bec50"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/sample_data/WA_Fn-UseC_-Telco-Customer-Churn.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3221441192.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/sample_data/WA_Fn-UseC_-Telco-Customer-Churn.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"İlk 5 satır:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/sample_data/WA_Fn-UseC_-Telco-Customer-Churn.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_telco_data(df):\n",
        "    df = df.copy()\n",
        "\n",
        "    # 1) Gereksiz kolon: customerID\n",
        "    if 'customerID' in df.columns:\n",
        "        df.drop('customerID', axis=1, inplace=True)\n",
        "\n",
        "    # 2) TotalCharges -> numerik\n",
        "    if 'TotalCharges' in df.columns:\n",
        "        df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
        "\n",
        "    # 3) Eksik değerleri sil (özellikle TotalCharges'tan gelen NaN'ler)\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "    # 4) Hedef değişken: Churn (No=0, Yes=1)\n",
        "    df['Churn'] = df['Churn'].map({'No': 0, 'Yes': 1})\n",
        "\n",
        "    # 5) Tenure_Group feature\n",
        "    bins = [0, 12, 48, 100]\n",
        "    labels = ['Yeni', 'Orta', 'Sadık']\n",
        "    df['Tenure_Group'] = pd.cut(\n",
        "        df['tenure'],\n",
        "        bins=bins,\n",
        "        labels=labels,\n",
        "        right=True,\n",
        "        include_lowest=True\n",
        "    )\n",
        "\n",
        "    # 6) Services_Count feature\n",
        "    service_cols = [\n",
        "        'PhoneService', 'MultipleLines',\n",
        "        'InternetService', 'OnlineSecurity', 'OnlineBackup',\n",
        "        'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies'\n",
        "    ]\n",
        "\n",
        "    # Bazı sütunlar eksik olursa patlamasın diye filtreleyelim\n",
        "    service_cols = [c for c in service_cols if c in df.columns]\n",
        "\n",
        "    def to_binary(x):\n",
        "        return 1 if x == 'Yes' else 0\n",
        "\n",
        "    bin_cols = []\n",
        "    for col in service_cols:\n",
        "        new_col = col + '_bin'\n",
        "        df[new_col] = df[col].apply(to_binary)\n",
        "        bin_cols.append(new_col)\n",
        "\n",
        "    df['Services_Count'] = df[bin_cols].sum(axis=1)\n",
        "\n",
        "    # 7) Advanced feature'lar\n",
        "    # MonthlyCharges / Service sayısı (0 bölmeye karşı koruma)\n",
        "    df['Monthly_per_Service'] = df['MonthlyCharges'] / df['Services_Count'].replace(0, 1)\n",
        "\n",
        "    # Tenure başına aylık ücret\n",
        "    df['Charge_per_Tenure'] = df['MonthlyCharges'] / df['tenure'].replace(0, 1)\n",
        "\n",
        "    # TotalCharges / tenure (ortalama aylık toplam ödeme)\n",
        "    df['Avg_Monthly_Total'] = df['TotalCharges'] / df['tenure'].replace(0, 1)\n",
        "\n",
        "    # 8) X, y ayırma\n",
        "    target = 'Churn'\n",
        "    y = df[target]\n",
        "    X = df.drop(target, axis=1)\n",
        "\n",
        "    # 9) Sayısal ve kategorik sütunları tespit et\n",
        "    numeric_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "    categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "    # 10) One-hot encoding\n",
        "    X_encoded = pd.get_dummies(X, columns=categorical_cols, drop_first=True)\n",
        "\n",
        "    return X_encoded, y, numeric_cols\n"
      ],
      "metadata": {
        "id": "o4rVnrw28hZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y, numeric_cols = preprocess_telco_data(df)\n",
        "\n",
        "print(\"X shape:\", X.shape)\n",
        "print(\"y shape:\", y.shape)\n",
        "print(\"Pozitif sınıf oranı (Churn=1):\", y.mean())\n",
        "print(\"\\nİlk 5 satır:\")\n",
        "display(X.head())\n",
        "\n",
        "print(\"\\nSayısal sütunlar (scale edeceğimiz kolonlar):\")\n",
        "print(numeric_cols)\n"
      ],
      "metadata": {
        "id": "TOjEk34p8sS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 1) Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "print(\"Train shape:\", X_train.shape)\n",
        "print(\"Test shape:\", X_test.shape)\n",
        "print(\"Churn oranı (train):\", y_train.mean())\n"
      ],
      "metadata": {
        "id": "ZGQFrolK9GYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "\n",
        "# sadece sayısal kolonları scale edeceğiz\n",
        "X_train_scaled = X_train.copy()\n",
        "X_test_scaled = X_test.copy()\n",
        "\n",
        "X_train_scaled[numeric_cols] = scaler.fit_transform(X_train[numeric_cols])\n",
        "X_test_scaled[numeric_cols] = scaler.transform(X_test[numeric_cols])\n",
        "\n",
        "print(\"Scale edilmiş X_train örnek:\")\n",
        "display(X_train_scaled.head())\n"
      ],
      "metadata": {
        "id": "WBrYvg9t9K8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE, ADASYN\n",
        "\n",
        "# SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)\n",
        "\n",
        "# ADASYN\n",
        "adasyn = ADASYN(random_state=42)\n",
        "X_train_adasyn, y_train_adasyn = adasyn.fit_resample(X_train_scaled, y_train)\n",
        "\n",
        "print(\"Orijinal veri:\", y_train.value_counts())\n",
        "print(\"\\nSMOTE sonrası:\", y_train_smote.value_counts())\n",
        "print(\"\\nADASYN sonrası:\", y_train_adasyn.value_counts())\n"
      ],
      "metadata": {
        "id": "4R9-KH3p9POf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, X_test, y_test, name=\"Model\"):\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    prec = precision_score(y_test, y_pred)\n",
        "    rec = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "    y_prob = model.predict_proba(X_test)[:, 1]\n",
        "    auc = roc_auc_score(y_test, y_prob)\n",
        "\n",
        "    print(f\"\\n{name} Skorları:\")\n",
        "    print(\"-\" * 40)\n",
        "    print(\"Accuracy:\", acc)\n",
        "    print(\"Precision:\", prec)\n",
        "    print(\"Recall:\", rec)\n",
        "    print(\"F1 Score:\", f1)\n",
        "    print(\"AUC:\", auc)\n",
        "\n",
        "    return {\n",
        "        \"Model\": name,\n",
        "        \"Accuracy\": acc,\n",
        "        \"Precision\": prec,\n",
        "        \"Recall\": rec,\n",
        "        \"F1\": f1,\n",
        "        \"AUC\": auc\n",
        "    }\n"
      ],
      "metadata": {
        "id": "3PXuE19x9T0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "\n",
        "# 1) Orijinal veri\n",
        "lr = LogisticRegression(max_iter=1000, random_state=42)\n",
        "lr.fit(X_train_scaled, y_train)\n",
        "results.append(evaluate_model(lr, X_test_scaled, y_test, \"LR - Original\"))\n",
        "\n",
        "# 2) SMOTE verisi\n",
        "lr_sm = LogisticRegression(max_iter=1000, random_state=42)\n",
        "lr_sm.fit(X_train_smote, y_train_smote)\n",
        "results.append(evaluate_model(lr_sm, X_test_scaled, y_test, \"LR - SMOTE\"))\n",
        "\n",
        "# 3) ADASYN verisi\n",
        "lr_ad = LogisticRegression(max_iter=1000, random_state=42)\n",
        "lr_ad.fit(X_train_adasyn, y_train_adasyn)\n",
        "results.append(evaluate_model(lr_ad, X_test_scaled, y_test, \"LR - ADASYN\"))\n"
      ],
      "metadata": {
        "id": "xCNS8TZ_-VFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Original\n",
        "rf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
        "rf.fit(X_train_scaled, y_train)\n",
        "results.append(evaluate_model(rf, X_test_scaled, y_test, \"RF - Original\"))\n",
        "\n",
        "# 2) SMOTE\n",
        "rf_sm = RandomForestClassifier(n_estimators=200, random_state=42)\n",
        "rf_sm.fit(X_train_smote, y_train_smote)\n",
        "results.append(evaluate_model(rf_sm, X_test_scaled, y_test, \"RF - SMOTE\"))\n",
        "\n",
        "# 3) ADASYN\n",
        "rf_ad = RandomForestClassifier(n_estimators=200, random_state=42)\n",
        "rf_ad.fit(X_train_adasyn, y_train_adasyn)\n",
        "results.append(evaluate_model(rf_ad, X_test_scaled, y_test, \"RF - ADASYN\"))\n"
      ],
      "metadata": {
        "id": "Icpj1wFA-jcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic XGBoost\n",
        "xgb = XGBClassifier(\n",
        "    n_estimators=300,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=5,\n",
        "    subsample=0.9,\n",
        "    colsample_bytree=0.9,\n",
        "    eval_metric=\"logloss\",\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "xgb.fit(X_train_scaled, y_train)\n",
        "results.append(evaluate_model(xgb, X_test_scaled, y_test, \"XGBoost - Original\"))\n"
      ],
      "metadata": {
        "id": "tnfJA34y-pbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df\n"
      ],
      "metadata": {
        "id": "ZY26PR3A-uRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LightGBM modelleri\n",
        "lgb_params = {\n",
        "    \"n_estimators\": 300,\n",
        "    \"learning_rate\": 0.05,\n",
        "    \"max_depth\": -1,\n",
        "    \"num_leaves\": 31,\n",
        "    \"random_state\": 42\n",
        "}\n",
        "\n",
        "# 1) Original\n",
        "lgb = LGBMClassifier(**lgb_params)\n",
        "lgb.fit(X_train_scaled, y_train)\n",
        "results.append(evaluate_model(lgb, X_test_scaled, y_test, \"LightGBM - Original\"))\n",
        "\n",
        "# 2) SMOTE\n",
        "lgb_sm = LGBMClassifier(**lgb_params)\n",
        "lgb_sm.fit(X_train_smote, y_train_smote)\n",
        "results.append(evaluate_model(lgb_sm, X_test_scaled, y_test, \"LightGBM - SMOTE\"))\n",
        "\n",
        "# 3) ADASYN\n",
        "lgb_ad = LGBMClassifier(**lgb_params)\n",
        "lgb_ad.fit(X_train_adasyn, y_train_adasyn)\n",
        "results.append(evaluate_model(lgb_ad, X_test_scaled, y_test, \"LightGBM - ADASYN\"))\n"
      ],
      "metadata": {
        "id": "8AJGjwlm_Fwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svm = SVC(kernel='rbf', probability=True, random_state=42)\n",
        "svm.fit(X_train_scaled, y_train)\n",
        "results.append(evaluate_model(svm, X_test_scaled, y_test, \"SVM\"))\n"
      ],
      "metadata": {
        "id": "64HbE_Ij_Jvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "xgb_base = XGBClassifier(\n",
        "    eval_metric=\"logloss\",\n",
        "    random_state=42,\n",
        "    use_label_encoder=False\n",
        ")\n",
        "\n",
        "param_grid = {\n",
        "    \"n_estimators\": [200, 300, 400, 500],\n",
        "    \"max_depth\": [3, 4, 5, 6, 7],\n",
        "    \"learning_rate\": [0.01, 0.05, 0.1, 0.2],\n",
        "    \"subsample\": [0.7, 0.8, 0.9, 1.0],\n",
        "    \"colsample_bytree\": [0.7, 0.8, 0.9, 1.0],\n",
        "    \"gamma\": [0, 1, 3, 5]\n",
        "}\n",
        "\n",
        "random_search_xgb = RandomizedSearchCV(\n",
        "    estimator=xgb_base,\n",
        "    param_distributions=param_grid,\n",
        "    n_iter=20,\n",
        "    scoring=\"recall\",\n",
        "    cv=3,\n",
        "    verbose=1,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "random_search_xgb.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(\"Best Params:\", random_search_xgb.best_params_)\n",
        "best_xgb = random_search_xgb.best_estimator_\n",
        "\n",
        "results.append(evaluate_model(best_xgb, X_test_scaled, y_test, \"XGBoost - Tuned\"))\n"
      ],
      "metadata": {
        "id": "JIhb9GrT_aKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "rf_base = RandomForestClassifier(random_state=42)\n",
        "\n",
        "rf_param_grid = {\n",
        "    'n_estimators': [200, 300, 400, 500],\n",
        "    'max_depth': [None, 10, 15, 20, 25],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['sqrt', 'log2']\n",
        "}\n",
        "\n",
        "rf_random_search = RandomizedSearchCV(\n",
        "    estimator=rf_base,\n",
        "    param_distributions=rf_param_grid,\n",
        "    n_iter=20,\n",
        "    scoring='recall',\n",
        "    cv=3,\n",
        "    verbose=1,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "rf_random_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(\"RF Best Params:\", rf_random_search.best_params_)\n",
        "best_rf = rf_random_search.best_estimator_\n",
        "\n",
        "results.append(evaluate_model(best_rf, X_test_scaled, y_test, \"RandomForest - Tuned\"))\n"
      ],
      "metadata": {
        "id": "dx8bFfHs_wNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df = pd.DataFrame(results)\n",
        "results_df.sort_values(\"Recall\", ascending=False)\n"
      ],
      "metadata": {
        "id": "2aEwxUIYA0l4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lime --quiet\n"
      ],
      "metadata": {
        "id": "eskrfwvcCvgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "from lime.lime_tabular import LimeTabularExplainer\n",
        "\n",
        "shap.initjs()\n",
        "\n",
        "feature_names = X.columns.tolist()\n",
        "print(\"SHAP ve LIME başarıyla yüklendi.\")"
      ],
      "metadata": {
        "id": "vro9FrtmDS0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# XGBoost Tuned için SHAP\n",
        "explainer_xgb = shap.TreeExplainer(best_xgb)\n",
        "\n",
        "# DataFrame yerine numpy array ile çalışmak daha stabil olabiliyor\n",
        "X_test_array = X_test_scaled.values\n",
        "\n",
        "shap_values_xgb = explainer_xgb(X_test_array)\n",
        "\n",
        "# Global önem grafiği\n",
        "shap.summary_plot(shap_values_xgb, X_test_array, feature_names=feature_names)\n"
      ],
      "metadata": {
        "id": "9eGmjUtSE81l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = 0  # istediğin index\n",
        "shap.plots.waterfall(shap_values_xgb[i])\n"
      ],
      "metadata": {
        "id": "aQqoxFpbFAS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LightGBM - SMOTE için SHAP\n",
        "explainer_lgb = shap.TreeExplainer(lgb_sm)\n",
        "X_test_array = X_test_scaled.values  # aynı array'i kullanıyoruz\n",
        "\n",
        "shap_values_lgb = explainer_lgb(X_test_array)\n",
        "\n",
        "shap.summary_plot(shap_values_lgb, X_test_array, feature_names=feature_names)\n",
        "\n",
        "i = 0\n",
        "shap.plots.waterfall(shap_values_lgb[i])\n"
      ],
      "metadata": {
        "id": "NSz1TjhUFEQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from lime.lime_tabular import LimeTabularExplainer\n",
        "\n",
        "lime_explainer = LimeTabularExplainer(\n",
        "    training_data=X_train_scaled.values,\n",
        "    feature_names=feature_names,\n",
        "    class_names=[\"No Churn\", \"Churn\"],\n",
        "    mode=\"classification\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "VS-_ZX3rFKVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = 0  # farklı satır denemek istersen değiştirirsin\n",
        "\n",
        "exp_lr = lime_explainer.explain_instance(\n",
        "    X_test_scaled.iloc[i].values,\n",
        "    lr_ad.predict_proba,\n",
        "    num_features=10\n",
        ")\n",
        "\n",
        "# Notebook içinde görsel göster\n",
        "exp_lr.show_in_notebook()\n",
        "\n",
        "# Konsolda liste halinde görmek istersen:\n",
        "exp_lr.as_list()\n"
      ],
      "metadata": {
        "id": "v0ojJ3_oFOJG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}